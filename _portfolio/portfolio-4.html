---
title: "EMODE Emotion Defender Game"
excerpt: "**_Affective_ _Computing_, _SFU_** <br/>Adding interaction to game using expression recognition"
collection: portfolio
period: "Jan 2024 – April 2024"
---

<section class="project">

    <p>
      <strong>Emotion Defender</strong> is a real-time game controlled entirely through
      <strong>facial expression recognition</strong>. Instead of keyboards or controllers, players
      interact using emotions detected from a webcam feed. This project investigates whether expression
      recognition can function as a <strong>primary interaction method</strong> in games rather than being
      used only for background adaptation.
    </p>
  
    <hr />
  
    <h2>Overview</h2>
    <p>
      The game integrates a webcam-based emotion recognition pipeline with a Python game engine.
      Different emotions correspond to different in-game effects, creating a novel and engaging way
      to play through facial expressions.
    </p>
  
    <h2>Motivation</h2>
    <p>
      Traditional controls can limit how naturally players express themselves. We explored a new form of
      human-computer interaction where <strong>emotions become the controller</strong>, asking:
      <em>Can facial expressions be the main way to play a game?</em>
    </p>
  
    <h2>System Architecture</h2>
    <ol>
      <li>Capture webcam frames using OpenCV</li>
      <li>Detect and crop the face region</li>
      <li>Convert to grayscale and resize to <strong>48×48</strong></li>
      <li>Predict emotion using a trained ML model</li>
      <li>Send prediction to the Pygame loop</li>
      <li>Trigger game effects based on the detected emotion</li>
    </ol>
    <p>
      This pipeline runs multiple times per second to support responsive gameplay.
    </p>
  
    <h2>Machine Learning Approaches</h2>
    <div class="table-wrap">
      <table class="proj-table">
        <thead>
          <tr>
            <th>Method</th>
            <th>Approach</th>
            <th>Description</th>
            <th>Result</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Method 1</td>
            <td>CNN (7 emotions)</td>
            <td>Conv2D + pooling + batch norm + dense layers</td>
            <td>~56% peak validation accuracy</td>
          </tr>
          <tr>
            <td>Method 1.2</td>
            <td>CNN (5 emotions)</td>
            <td>Trained only on emotions used in the game</td>
            <td>Improved some classes; anger/sadness remained weak</td>
          </tr>
          <tr>
            <td><strong>Method 2</strong></td>
            <td><strong>Fine-tuned CNN</strong></td>
            <td>Transfer learning from Method 1, fine-tuned on 5 emotions</td>
            <td><strong>83.3% test accuracy</strong> (best)</td>
          </tr>
          <tr>
            <td>Method 3</td>
            <td>PCA + SVM</td>
            <td>Dimensionality reduction (150 components) + SVM (RBF)</td>
            <td>48% accuracy</td>
          </tr>
        </tbody>
      </table>
    </div>
    <p>
      The fine-tuned CNN achieved the best performance and the lowest misclassification across emotion categories.
    </p>
  
    <h2>Dataset</h2>
    <ul>
      <li>FER-2013 facial expression dataset (7 emotion classes)</li>
      <li>Custom labeled photos of team members and friends</li>
      <li>Preprocessing: grayscale, 48×48 resolution</li>
    </ul>
    <p>
      Interestingly, training with all seven emotions improved class separation even though the final game used five.
    </p>
  
    <h2>Key Findings</h2>
    <ul>
      <li><strong>Happiness</strong> and <strong>surprise</strong> were consistently easier to detect than anger/sadness</li>
      <li>Anger/sadness were harder due to subtle facial cues and confusion between classes</li>
      <li>Merging fear/disgust predictions into anger improved usability in real-time play</li>
      <li>High prediction frequency helped compensate for imperfect accuracy</li>
    </ul>
  
    <h2>Outcome</h2>
    <p>
      Emotion Defender demonstrates that facial expression recognition can be a
      <strong>viable and engaging primary interaction method</strong> for games, combining computer vision,
      machine learning, and interactive design into a novel gameplay experience.
    </p>
  
    <h2>Tech Stack</h2>
    <p>
      Python · OpenCV · TensorFlow/Keras · scikit-learn · Pygame · Matplotlib/Seaborn
    </p>
  
    <hr />
  
    <p class="links">
        <a href="/nagisa.github.io/images/Emodefinalreport.pdf" target="_blank">
            Final Report (PDF)
        </a>
    </p>
  
  </section>
  
  <style>
    /* Minimal styling (works well with Hydejack too) */
    .project { margin-top: 1rem; }
    .project hr { margin: 1.5rem 0; opacity: 0.25; }
    .project h2 { margin-top: 1.6rem; }
    .table-wrap { overflow-x: auto; margin: 0.75rem 0 0.5rem; }
    .proj-table { width: 100%; border-collapse: collapse; }
    .proj-table th, .proj-table td { padding: 10px 12px; border: 1px solid rgba(0,0,0,0.12); vertical-align: top; }
    .proj-table th { text-align: left; }
    .links a { font-weight: 600; }
  </style>